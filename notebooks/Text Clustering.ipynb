{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering using Enron Emails\n",
    "\n",
    "Unsupervised clustering using K-Means. This algorithm searches for a pre-determined number of clusters within a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import email\n",
    "import spacy\n",
    "import textacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import sklearn\n",
    "import mglearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nlp = spacy.load('en')  # spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>\n",
      "Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: tim.belden@enron.com\n",
      "Subject: \n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: Tim Belden <Tim Belden/Enron@EnronXGate>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "Here is our forecast\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "df = pd.read_csv('../data/emails.csv')\n",
    "print(df.iloc[0]['message'])  # Print what the data contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Mime-Version</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Content-Transfer-Encoding</th>\n",
       "      <th>X-From</th>\n",
       "      <th>X-To</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>content</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Mon, 14 May 2001 16:39:00 -0700 (PDT)</td>\n",
       "      <td>(phillip.allen@enron.com)</td>\n",
       "      <td>(tim.belden@enron.com)</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Tim Belden &lt;Tim Belden/Enron@EnronXGate&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>pallen (Non-Privileged).pst</td>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "      <td>allen-p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>&lt;15464986.1075855378456.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Fri, 4 May 2001 13:51:00 -0700 (PDT)</td>\n",
       "      <td>(phillip.allen@enron.com)</td>\n",
       "      <td>(john.lavorato@enron.com)</td>\n",
       "      <td>Re:</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>John J Lavorato &lt;John J Lavorato/ENRON@enronXg...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>pallen (Non-Privileged).pst</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>allen-p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     file                                     Message-ID  \\\n",
       "0   allen-p/_sent_mail/1.  <18782981.1075855378110.JavaMail.evans@thyme>   \n",
       "1  allen-p/_sent_mail/10.  <15464986.1075855378456.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    Date                       From  \\\n",
       "0  Mon, 14 May 2001 16:39:00 -0700 (PDT)  (phillip.allen@enron.com)   \n",
       "1   Fri, 4 May 2001 13:51:00 -0700 (PDT)  (phillip.allen@enron.com)   \n",
       "\n",
       "                          To Subject Mime-Version  \\\n",
       "0     (tim.belden@enron.com)                  1.0   \n",
       "1  (john.lavorato@enron.com)     Re:          1.0   \n",
       "\n",
       "                   Content-Type Content-Transfer-Encoding           X-From  \\\n",
       "0  text/plain; charset=us-ascii                      7bit  Phillip K Allen   \n",
       "1  text/plain; charset=us-ascii                      7bit  Phillip K Allen   \n",
       "\n",
       "                                                X-To X-cc X-bcc  \\\n",
       "0           Tim Belden <Tim Belden/Enron@EnronXGate>              \n",
       "1  John J Lavorato <John J Lavorato/ENRON@enronXg...              \n",
       "\n",
       "                                            X-Folder X-Origin  \\\n",
       "0  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...  Allen-P   \n",
       "1  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...  Allen-P   \n",
       "\n",
       "                    X-FileName  \\\n",
       "0  pallen (Non-Privileged).pst   \n",
       "1  pallen (Non-Privileged).pst   \n",
       "\n",
       "                                             content     user  \n",
       "0                          Here is our forecast\\n\\n   allen-p  \n",
       "1  Traveling to have a business meeting takes the...  allen-p  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Data parsing and loading functions based from Zichen -- thank you for sharing your exploration!\n",
    "Using this to better keep the information if we want it later it's easier to process. \n",
    "https://www.kaggle.com/zichen/explore-enron\n",
    "'''\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs\n",
    "\n",
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, df['message']))\n",
    "df.drop('message', axis=1, inplace=True)\n",
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    df[key] = [doc[key] for doc in messages]\n",
    "# Parse content from emails\n",
    "df['content'] = list(map(get_text_from_email, messages))\n",
    "# Split multiple email addresses\n",
    "df['From'] = df['From'].map(split_email_addresses)\n",
    "df['To'] = df['To'].map(split_email_addresses)\n",
    "\n",
    "# Extract the root of 'file' as 'user'\n",
    "df['user'] = df['file'].map(lambda x:x.split('/')[0])\n",
    "del messages\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_preprocess(doc):\n",
    "    \"\"\" Function to clean and run the text pre-processing steps.\n",
    "    1. Lowercase\n",
    "    2. Punctuation\n",
    "    3. Stopwords\n",
    "    :param doc: string\n",
    "    :returns: text\n",
    "    \"\"\"\n",
    "    # Lowercase -- can be done in spacy doc object as doc.lower_\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Punctuation\n",
    "    doc = textacy.preprocess.remove_punct(doc)\n",
    "    \n",
    "    # Stemming/lemmatization\n",
    "    #doc = [w.lemma_ for w in nlp(doc)]\n",
    "    \n",
    "    # Stopwords (using spacy stopwords)\n",
    "    doc = [w for w in doc.split(' ') if not w in spacy.en.STOP_WORDS]\n",
    "    \n",
    "    # Custom stopwords to remove\n",
    "    rm = ['-PRON-']\n",
    "    doc = [w for w in doc if not w in rm]\n",
    "    \n",
    "    # Additional normalization using textacy\n",
    "    # doc = [textacy.preprocess_text(w) for w in doc]\n",
    "    \n",
    "    string = ' '.join(doc).replace('\\n\\n',' ').replace('\\n', ' ')\n",
    "    return string\n",
    "    #return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.6 s, sys: 740 ms, total: 57.4 s\n",
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%time df['processed_content'] = df.content.apply(clean_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english')\n",
    "x = v.fit_transform(df['processed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.get_feature_names()[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "clf = KMeans(n_clusters=n)\n",
    "labels = clf.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.externals.joblib.dump(labels, 'kmeans.pkl')\n",
    "#labels = sklearn.externals.joblib.load('kmeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster memberships\n",
    "print(labels.labels_[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.1 s, sys: 4.7 s, total: 39.8 s\n",
      "Wall time: 40.5 s\n"
     ]
    }
   ],
   "source": [
    "# Minibatch kmeans\n",
    "clf_mb = MiniBatchKMeans(n_clusters=2)\n",
    "%time mb_labels = clf_mb.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "labels_mb = clf_mb.labels_\n",
    "centr = clf_mb.cluster_centers_\n",
    "    \n",
    "silhouette_score(x, labels_mb, metric='euclidean')  # runs out of memory each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(s)\n",
    "plt.ylabel('Silhouette')\n",
    "plt.xlabel('K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Terms per cluster\n",
    "\n",
    "Resources: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': v.get_feature_names()[:]}, index = v.get_feature_names()[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = clf.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :20]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[v.get_feature_names()[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(x)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=2, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = []\n",
    "\n",
    "for n_clusters in range(2,4):\n",
    "    k_mini = MiniBatchKMeans(n_clusters=n_clusters)\n",
    "    k_mini.fit(x)\n",
    "    \n",
    "    labels = k_mini.labels_\n",
    "    centr = k_mini.cluster_centers_\n",
    "    \n",
    "    s.append(silhouette_score(x, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(s)\n",
    "plt.ylabel('Silhouette')\n",
    "plt.xlabel('K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Resources\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_clustering.html\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py\n",
    "\n",
    "http://brandonrose.org/clustering\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "\n",
    "https://www.kaggle.com/jaykrishna/topic-modeling-enron-email-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
